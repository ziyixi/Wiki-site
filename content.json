{"pages":[{"title":"About","date":"2019-06-11T18:29:32.881Z","path":"about/index.html","text":""},{"title":"Categories","date":"2019-06-11T18:29:32.882Z","path":"categories/index.html","text":""},{"title":"Tags","date":"2019-06-11T18:29:32.882Z","path":"tags/index.html","text":""}],"posts":[{"title":"Files in ICER and Stampede2","date":"2019-07-16T14:17:46.000Z","path":"wiki/files-in-hpcc/","text":"Overview Managing files in ICER and STAMPEDE2 is a important and I am supposed to find a way to place my files in order. That the reason why I write this wiki. The summary of the storage space In ICER, we have these four &quot;disks&quot;: (directly copied) 12345678910111213141516171819Home Directory:---------------------------------------------------------------------------------------------------------------------/mnt/home/xiziyi Quota Used Free Files Quota Files Used Files Free 1024G 251G 773G 1048576 988008 60568Research Groups: Space Space Space Files Files Files Quota Used Available Quota Used Available------------------------------------------------------------------------------------------------------------------seismolab2 16384G 10797G 5587G 26214400 24725040 1489360Temporary Filesystems:---------------------------------------------------------------------------------------------------------------------/mnt/ls15 (legacy scratch) Inodes Used Quota Free 115170 1000000 884830/mnt/scratch (/mnt/gs18) Space Quota Space Used Space Free Filess Quota Files Used Files Free 51200G 340G 50860G 1048576 71699 976877 And in stampede2, we have: 123456789--------------------- Project balances for user tg851791 ----------------------| Name Avail SUs Expires | Name Avail SUs Expires || TG-EAR140030 31763 2019-09-30 | TG-EAR130011 87905 2019-09-30 |------------------------ Disk quotas for user tg851791 ------------------------| Disk Usage (GB) Limit %Used File Usage Limit %Used || /home1 2.0 10.0 19.52 46696 200000 23.35 || /work 807.4 1024.0 78.85 2742645 3000000 91.42 || /scratch 23904.2 0.0 0.00 11820127 0 0.00 |------------------------------------------------------------------------------- Plan For the research purpose, we have the following data: 1. the models, which are used to run in Specfem and for visulization. (~50 GB) 2. the data, we should keep the raw data and the processed data. (raw data: ~1 TB) 3. the sync result. Now I have these different types of sync: + 484 events for the hybrid model. (each iteration takes the 0.5 TB space) + 30 events for validation models. (total ~100 GB space each iteration) + several events for the cmt correcction. (~200 events for each iteration. 8x space than sync, so about 1.6T) Assume we will run 20 iterations, then the total space will be: 1. The models, 50GB. 2. The data, 1TB. 3. The sync, since we will use ~200 events in each iteration, the total will be 4 TB. 4. The validation, since we will do that in each 5 iterations, the total will be 6.4 TB. 5. Kernels and Line search? COnsider them in the future. So we will need about 12 TB space or more. So we should store any of us data in ICER's research directory and compress them accordingly. (use asdf) As for the stampede2, we have only ~1T space. This is good for placing all the working directories. (only the structure of running specfem, and we can place the output and DATABASE_MPI in the scratch directory) The data structure. ICER /mnt/home/xiziyi anaconda bin data package script temp test /mnt/research/seismolab2/japan_slab cmts data models sync relocation For the data directory, we should place the directories as: raw (the seed files, should be renamed according to the gcmt id) fnet cea kma fdsn processed (after having preprocessed) bandpass_10s_120s bandpass_20s_120s bandpass_40s_120s For the sync directory, we have: hybrid first_iteration (sync should have the similar structure with data) ... validation EARA2014 (have the same structue with the hybrid) FWEA18 ... For the relocation directory, we have: depth (as we may have other types of relocation) frist_relocation_for_xxx_iteration. (asdf files are named according to there depth, also ln the 0 depth to the sync)","tags":[{"name":"stampede2,icer","slug":"stampede2-icer","permalink":"wiki.ziyixi.science/tags/stampede2-icer/"}],"categories":[{"name":"hpcc","slug":"hpcc","permalink":"wiki.ziyixi.science/categories/hpcc/"}]},{"title":"汽车购买","date":"2019-06-16T14:35:45.000Z","path":"wiki/buy-car/","text":"买车网站","tags":[{"name":"cars","slug":"cars","permalink":"wiki.ziyixi.science/tags/cars/"}],"categories":[{"name":"life","slug":"life","permalink":"wiki.ziyixi.science/categories/life/"}]},{"title":"lasif_specfem","date":"2019-06-14T14:46:24.000Z","path":"wiki/lasif-specfem/","text":"Overview I'm planning to build a package named LASIF_Specfem. This package aims to create a command line tool to manage the usage of Specfem-globe. As we know that Specfem is kind of complex, and rewrite it to the structure of what a modern software should look like is difficult and complex. But anyway, we can write a python interface to it. According to LASIF_2.0, we can use directory structures to manage our data, synthetics and more. There have already been some tools, but the problem is that these tools have some default setting which instructs you each steps. Since the scientific research is really flexble, such a restriction is not tolerable. So I'm planning to use &quot;plugins&quot; to solve this problem. The package LASIF_Specfem will only contain prototypes and set up the directory structure, and users can implement their own plugins following some interfaces.","tags":[{"name":"FWI","slug":"FWI","permalink":"wiki.ziyixi.science/tags/FWI/"}],"categories":[{"name":"FWI","slug":"FWI","permalink":"wiki.ziyixi.science/categories/FWI/"}]},{"title":"correct_cea_orientation","date":"2019-06-13T21:00:07.000Z","path":"wiki/correct-cea-orientation/","text":"Overview The cea data has mis-orientated before 09/2013. We have the document of how these stations are mis-orientated. (From Prof. Kai Tao and by Prof. Fenglin Niu) Dr. Chen has some data that have been processed before. So I'd like to compare the cmpaz of her processed data and the document mentioned above. Result The data from the document has the form 1#NET|STA|NO.EVENT|MEAN|STD|MEDIAN|MAD|STARTTIME|ENDTIME AH.BAS 1AH|BAS|130|8.46|10.14|10.23|4.50|2008-03-11T14:37:00Z|2010-04-11T09:40:00Z cmpaz = 1.000000e+01 AH.BEB 1AH|BEB|113|-4.82|20.58|-7.31|8.67|2007-10-02T03:43:00Z|2011-01-13T16:16:00Z cmpaz = -7.000000e+00 AH.DYN 1AH|DYN|101|66.71|6.26|66.86|3.83|2007-08-27T17:11:00Z|2008-11-03T19:21:00Z cmpaz = 6.500000e+01 AH.FZL 123456789#NET=AH,STA=FZL,MINCC=0.80,XYFILE=AH.FZL.XY#NET|STA|NO.EVENT|MEAN|STD|MEDIAN|MAD|STARTTIME|ENDTIMEAH|FZL|3|-39.87|38.49|-60.75|3.94|2007-08-08T17:04:00Z|2007-08-15T20:22:00ZAH|FZL|18|-20.44|29.88|-30.98|16.20|2007-09-12T11:10:00Z|2008-03-26T20:06:00ZAH|FZL|3|3.09|87.01|14.27|103.34|2008-04-02T14:36:00Z|2008-04-10T04:29:00ZAH|FZL|1|3.69|NA|3.69|0.00|2008-04-29T19:10:00Z|2008-04-29T19:10:00ZAH|FZL|327|3.04|11.37|1.71|7.47|2008-05-02T01:33:00Z|2013-09-01T11:52:00ZAH|FZL|0|NA|NA|NA|NA|NA|2013-09-01T11:52:00ZAH|FZL||||||2013-09-01T11:52:00Z|NULL cmpaz = -1.000000e+00 Here we don't have the data of the for the event 200804161919A. AH.HBE 1AH|HBE|64|13.63|5.46|13.70|3.91|2007-08-01T17:08:00Z|2008-06-10T04:13:00Z AH.HEF 1AH|HEF|58|-9.22|5.11|-9.29|3.71|2007-10-31T13:44:00Z|2010-02-01T22:28:00Z cmpaz = -9.000000e+00 AH.HSH 1AH|HSH|108|-183.73|12.04|-180.53|5.63|2007-09-12T23:49:00Z|2010-05-27T17:14:00Z cmpaz = -1.790000e+02 AH.HUS 1AH|HUS|59|-4.29|4.56|-4.26|2.56|2007-08-01T17:08:00Z|2008-07-06T01:00:00Z cmpaz = 0.000000e+00 AH.JZA 1AH|JZA|96|0.13|16.51|2.94|9.54|2007-08-08T17:04:00Z|2010-03-05T16:07:00Z cmpaz = 1.000000e+00 Conclusion It seems the mis-orientation data that Dr. Chen has used is slightly different from Dr. Tao has used. But they are similar to each other, only several degrees different.","tags":[{"name":"cea","slug":"cea","permalink":"wiki.ziyixi.science/tags/cea/"}],"categories":[{"name":"Research note","slug":"Research-note","permalink":"wiki.ziyixi.science/categories/Research-note/"}]},{"title":"SeisScript","date":"2019-06-11T21:07:03.000Z","path":"wiki/SeisScript/","text":"Overview Recently I have collected some of my research scripts into a package (or just a directory) named SeisScript. This package is very useful for the need of full waveform inversion in the study of seismology. Here I'd like to introduce different parts. specfem_helper.jl At the moment, this package contains two different usable programs. (and more usable functions!) They are all inside the src/program subdirectory. Before using them, we may have to enter the fortran directory to run julia compile.jl to compile two fortran files into the dynamic library so that we can use ccall to call them. (I can write them in pure Julia, but I'm pretty lazy.) Also we have to edit setting/constants.jl to edit ANGULAR_WIDTH_XI_IN_DEGREES_VAL, ANGULAR_WIDTH_ETA_IN_DEGREES_VAL, NEX_XI_VAL and NEX_ETA_VAL. The first program is named as xsem_interp_mesh2.jl, direcctly adapted from Tao Kai's code. But I have used some packages directly from Julia which makes the code more neat and more beautiful. According to my test, this program will have the same result with Tao's code, but three time slower. (That's reasonable, since Julia seems to be slower than Fortran, but I'm planning to precompile some parts to make it faster.) The second program is named as get_cross_section_data.jl. This program is mainly used to calculated the point values along a specific profile of the GLL model. Previously we have a script which is designed to find the nearest points along the interface of the profile. This method is somehow not accurate. So I'm using the interpolation to calculate the values. Comparing with the previous code, this program runs faster and uses less computing resources.","tags":[{"name":"FWI","slug":"FWI","permalink":"wiki.ziyixi.science/tags/FWI/"}],"categories":[{"name":"FWI","slug":"FWI","permalink":"wiki.ziyixi.science/categories/FWI/"}]},{"title":"Comparision of different filters in preprocessing data","date":"2019-06-11T21:07:03.000Z","path":"wiki/taper/","text":"Filters In the seismic research, we have to preprocess the data. Specially in the field of FWI, we may have to compare the synthetics and the data, and then calculate the misfit function. Here I compare two different way to get our desired frequency range. Bandpass The data we have the data and the PZ file. And then we can use SAC to preprocess it: 1234r AH.DYN.BHZ.trimrmean; rtr; taper;trans from polezero s ./AH.DYN.BHZ.pz to none freq 0.004 0.005 1 1.2bp n 2 p 2 c 0.01 0.0025 So here we will get the processed data with the frequency range of 40s to 100s. Taper in the frequency domain We can also directly get the final result as: 123r AH.DYN.BHZ.trimrmean; rtr; taper;trans from polezero s ./AH.DYN.BHZ.pz to none freq 0.008 0.01 0.025 0.03 Result We can compare the result as: As we can see, the black line represents &quot;taper from 0.004HZ to 0.005HZ&quot; and then removing the instrumental response, while the blue line represents &quot;taper from 0.01HZ to 0.0025HZ&quot; and then removing the instrumental response. And the red line represnets apply a bandpass filter from 0.01HZ to 0.0025HZ to the black line. In FWI research, we want to keep as much as energy in our desired frequency band. So it seems the blue has not losed the energy, which is better then the red line. Also the blue line has almost the same amount of enery with the red line. However, there is an abrupt jump of the blue line, which may cause some artifact. Look at part from 800s to 1000s. It seems the &quot;phases&quot; in the black line are the artifact, which may cause some problem for the comparision of the surface wave. Anyway, the behavior of sync and data outside our desired frequency range is somehow different.","tags":[{"name":"filter","slug":"filter","permalink":"wiki.ziyixi.science/tags/filter/"}],"categories":[{"name":"DSP","slug":"DSP","permalink":"wiki.ziyixi.science/categories/DSP/"}]},{"title":"未来职业规划","date":"2019-06-11T18:49:41.000Z","path":"wiki/future_plan1/","text":"谈到我自己的未来职业规划，我这里有一些比较初步的想法。 首先的一个关键问题在于我今后应该从事学术界的工作还是工业界的工作，这一点非常重要。从我本科时候的经历来看，要想在每一个人生阶段结束的时候达到一个更好的位置，就应该提早做准备。（譬如我自己出国这件事情其实是在大三下学期才开始正式开始准备的，这一点无疑成为我申请时候的劣势）此外我们还应该明白在每一阶段自己的核心竞争力是什么。在 PhD 阶段，我个人认为还是要着眼于科研。第一点在于做了些什么东西，即所做工作的数量。内行毕竟是少数，别人评价自己都要从一些标准的评价手段来看。（主要是发了些什么文章，在什么样的期刊上发了文章。）其次是所做的领域。（是否代表了学科的发展方向，是否具有真正的价值。）最后是所做工作的延伸性。（如何为自己创造更多的机会。） 回顾我现在所做的事情，大约在今年结束之前能把模型计算出来，在明年暑假之前文章应该可以发表。我自己所做的另一个算法方面的工作，我预计是能发一篇比较小的 paper。所以在前两年（或者三年），我应该能有两篇一作文章到手。再加上蹭师兄的两篇二作文章，我在前两年就会有四篇文章。从文章数目来看，我觉得似乎还是可以的。但是考虑到自己学校的问题，MSU 毕竟不能为我找工作提供助力。如果我想在国内找工作的话，所能凭借的有如下几点：教育背景：科大毕业，海外 PhD。学科背景：理科专业，cs dual，HPC 相关。我个人觉得这些条件应该能让我达到各大企业的基本入门要求，所以从这一点来看找工业界的工作还是有一些机会的。 如果我要从事学术界的工作的话，首先一点在于学术界的内卷现象。虽然说如果自己有比较好的 publication 的话，还是比较有希望在 PhD 毕业的时候找到一个比较好的博后位置，以至于再走教职的常规路线。理论上讲我是能走这条路的，而且走这条路的各个时期所要做的事情都是可以预期的。可是走这条路的一个大问题在于竞争激烈。不用脑子想我也知道业界待遇和职位数额要远远大于学术界。从我的观点来看，不转行的一部分在于确实拥有着远大的学术理想。但是更多的我觉得是不能跳出自己的舒适区。而从我个人发展的角度来看，我或者从我个人的兴趣来看，我其实还是比较喜欢做一些基础性的 coding 工作的，而且经过一直以来对学术界和业界的对比来看，我觉得业界还是更能实现我个人价值的地方。 如果从事业界工作，什么能力是我需要具备的？ 基础知识。cs 本科很多东西我都是水水而过，我个人觉得还需要加强。此外现在做 cs 相关工作，还有很多知识需要加以补充，具体而言，我觉得有如下事情需要去做： 离散数学。Rosen_Discrete_Mathematics_and_Its_Applications_7th_Edition 算法。 算法导论 C++。cpp 是编程的基本功，不是学学 py 或者 Julia 能够取代的。而且自然我应该重读一下 C++ primer。 操作系统。这一科我建议直接阅读 computer systems 一书。我觉得可以和计算机系统一课合并。 数据库。 直接学习 SQL 学习指南这一动物书。 计算机网络。 这一科其实我不是非常喜欢，暂且扔掉。 编译原理。 学习 engineering a compiler 一书。 综上所述，我一共需要阅读六本书。因为我之前有 cs 的基础，所以除了算法导论一书外我觉得其余的书都可以在一个月内读完。算法导论需要阅读两到三个月之间。 进阶知识。我希望从事的方向是偏向机器学习的高性能计算方面的工作，比如设计 GPU 算法之类的活。我自认为我在这一方面是比较有学习优势的。这就要求我在机器学习和高性能计算方面两手抓。 GPU 基础知识。我需要知道如何使用 GPU，也需要知道其底层的原理。甚至对于 tpu 的相关知识，我也应该加以学习。我觉得可以阅读 Programming massively parallel processors: A hands-on approach 一书。 CUDA 编程。我希望熟练掌握 c++，py 和 Julia 的 CUDA 编程。暂且阅读 CUDA C programming 一书。 学习“系统研究背景：操作系统、移动计算、边缘计算、分布式系统、硬件等”。这个日后详谈。 学习数电与 FPGA。 Anyway，我需要每天确保一定的学习时间，以便在明年暑假实习的时候达到应有的 cs 科班水平。此外应当关注科大的微软亚洲研究院的教授，以便参加 MSRA 的暑假实习。","tags":[],"categories":[{"name":"所思所想","slug":"所思所想","permalink":"wiki.ziyixi.science/categories/所思所想/"}]}]}